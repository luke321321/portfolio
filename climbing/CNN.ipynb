{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we train a convolution neural network (based upon ResNetv2) to classify routes by their grades and achieve 62% accuracy on the test dataset. \n",
    "\n",
    "### Building the data pipeline\n",
    "We load the input data, create the training, validation and testing datasets ensuring the proper distribution of grade 6, 7 and 8's in each, and then build the data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.utils import io\n",
    "\n",
    "from skopt import gp_minimize, callbacks, load\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.callbacks import CheckpointSaver\n",
    "# Need skopt.__version__ > 0.5.2 or pip install git+https://github.com/scikit-optimize/scikit-optimize/\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rcParams[\"image.origin\"] = 'lower'\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (8789, 3, 18, 11) float32\n",
      "Train labels shape:  (8789,) int32\n",
      "Validation data shape:  (1097, 3, 18, 11)\n",
      "Validation labels shape:  (1097,)\n",
      "Test data shape:  (1097, 3, 18, 11)\n",
      "Test labels shape:  (1097,)\n"
     ]
    }
   ],
   "source": [
    "def load_data(train=0.8, val=0.1, test=0.1, data_format='channels_last'):\n",
    "    \"\"\"\n",
    "    Loads the datasets from data/data.npz and randomly creates the train, test and \n",
    "    validation datasets.\n",
    "\n",
    "    Inputs:\n",
    "    - train, val, test: the fraction of the dataset in the train dataset, validation dataset\n",
    "      and test dataset respectively\n",
    "    - data_format: A string, one of 'channels_last' (default) or 'channels_first'.\n",
    "      'channels_last' best for CPU and 'channels_first' best for GPU\n",
    "    Returns:\n",
    "    - 6 numpy arrays: X_train, y_train, X_val, y_val, X_test, y_test in either\n",
    "      channel_last or channel_first format\n",
    "    - grade_dict : a dictionary of coverting the grades to numerical scores.\n",
    "    \"\"\"\n",
    "    assert train + val + test == 1\n",
    "\n",
    "    # load the data, n.b. arrays are sorted by grade\n",
    "    loaded = np.load('data/data.npz')\n",
    "    moves = loaded['moves']\n",
    "    grades = loaded['grades']\n",
    "    grade_dict = loaded['grade_dict'][()]\n",
    "\n",
    "    # Find partition arguments between grade 6, 7 & 8\n",
    "    part_arg = np.searchsorted(grades, [grade_dict['7A'], grade_dict['8A']])\n",
    "\n",
    "    # now shuffle within the grade 6's, 7's and 8's\n",
    "    permute_idx = np.arange(grades.shape[0])\n",
    "    np.random.shuffle(permute_idx[:part_arg[0]])\n",
    "    np.random.shuffle(permute_idx[part_arg[0]:part_arg[1]])\n",
    "    np.random.shuffle(permute_idx[part_arg[1]:])\n",
    "    moves = moves[permute_idx]\n",
    "    grades = grades[permute_idx]\n",
    "\n",
    "    # data processing\n",
    "    if data_format == 'channels_first':\n",
    "        moves = np.moveaxis(moves, -1, 1)\n",
    "    moves = moves.astype(np.float32)\n",
    "\n",
    "    # create the train, val and test datasets from the grade classes\n",
    "    part_start = np.append(0, part_arg)\n",
    "    size = np.array([part_arg[0], part_arg[1] - part_arg[0],\n",
    "                     len(grades) - part_arg[1]])\n",
    "\n",
    "    num_val = (val * size).astype(int)\n",
    "    num_test = (test * size).astype(int)\n",
    "    num_train = (size - num_val - num_test).astype(int)\n",
    "\n",
    "    # generate the training, val and test sets\n",
    "    slice_range = [part_start,\n",
    "                   part_start + num_train,\n",
    "                   part_start + num_train + num_val,\n",
    "                   part_start + num_train + num_val + num_test]\n",
    "    X, y = [], []\n",
    "    for j in range(3):\n",
    "        grade_list, moves_list = [], []\n",
    "        for i in range(3):\n",
    "            grade_list.append(grades[slice_range[j][i]: slice_range[j+1][i]])\n",
    "            moves_list.append(moves[slice_range[j][i]: slice_range[j+1][i]])\n",
    "        X.append(np.concatenate(moves_list))\n",
    "        y.append(np.concatenate(grade_list))\n",
    "\n",
    "    X_train, X_val, X_test = X\n",
    "    y_train, y_val, y_test = y\n",
    "\n",
    "    # check: sets are the correct length\n",
    "    assert (len(y_val) == np.sum(num_val) and len(y_test) == np.sum(num_test)\n",
    "            and len(y_train) == np.sum(num_train))\n",
    "\n",
    "    # Normalize the data: subtract the mean pixel and divide by std\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, grade_dict\n",
    "\n",
    "\n",
    "data_format = 'channels_first'\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, grade_dict = load_data(\n",
    "    data_format=data_format)\n",
    "num_classes = len(grade_dict) - 1\n",
    "\n",
    "print('Train data shape: ', X_train.shape, X_train.dtype)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "\n",
    "def construct_datasets(num_epochs=1):\n",
    "    \"\"\"\n",
    "    Constructs the datasets in Tensorflow.\n",
    "\n",
    "    Inputs: \n",
    "    - num_epochs: The number of epochs to run the training data for\n",
    "\n",
    "    Outputs:\n",
    "    - next_element_train, next_element_test: get_next() method for train and test dataset iterators.\n",
    "      The next_element_test is either from the validation or testing dataset depending on which has\n",
    "      been initialised\n",
    "    - train_init_op, val_init_op, test_init_op:\n",
    "      iterator initialisation operations for the respective datasets\n",
    "    - steps_to_epochs: The number of integers steps to each epoch of the training dataset\n",
    "    \"\"\"\n",
    "    prefetch = 2\n",
    "\n",
    "    # make sure the dataset is on the CPU to leave the GPU for training the model\n",
    "    with tf.device('/cpu:0'):\n",
    "        with tf.variable_scope('train_dataset'):\n",
    "            dataset_train = tf.data.Dataset.from_tensor_slices(\n",
    "                (X_train, y_train))\n",
    "            dataset_train = dataset_train.apply(\n",
    "                tf.data.experimental.shuffle_and_repeat(len(X_train), count=num_epochs))\n",
    "            dataset_train = dataset_train.shuffle(len(X_train))\n",
    "            dataset_train = dataset_train.batch(batch_size).prefetch(prefetch)\n",
    "\n",
    "        with tf.variable_scope('validation_dataset'):\n",
    "            dataset_val = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "            dataset_val = dataset_val.batch(batch_size).prefetch(prefetch)\n",
    "        with tf.variable_scope('test_dataset'):\n",
    "            dataset_test = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "            dataset_test = dataset_test.batch(batch_size).prefetch(prefetch)\n",
    "\n",
    "        iterator_train = tf.data.Iterator.from_structure(dataset_train.output_types,\n",
    "                                                         dataset_train.output_shapes)\n",
    "        next_element_train = iterator_train.get_next()\n",
    "        iterator_test = tf.data.Iterator.from_structure(dataset_train.output_types,\n",
    "                                                        dataset_train.output_shapes)\n",
    "        next_element_test = iterator_test.get_next()\n",
    "\n",
    "        train_init_op = iterator_train.make_initializer(dataset_train)\n",
    "        val_init_op = iterator_test.make_initializer(dataset_val)\n",
    "        test_init_op = iterator_test.make_initializer(dataset_test)\n",
    "        steps_to_epochs = len(X_train) // batch_size\n",
    "\n",
    "    return next_element_train, next_element_test, train_init_op, val_init_op, test_init_op, steps_to_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the neural network\n",
    "\n",
    "Our neural network is a deep network based upon Resnetv2 and has the same structure as the CIFAR-10 version of ResNetv2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "# Helper layer functions\n",
    "\n",
    "\n",
    "def batch_norm_relu_conv2d(inputs, filters, is_training, stride=1, reg=1e-4, data_format='channels_last'):\n",
    "    inputs = batch_norm_relu(inputs, is_training)\n",
    "    inputs = conv2d(inputs, filters, is_training, stride=stride,\n",
    "                    reg=reg, data_format=data_format)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def batch_norm_relu(inputs, is_training):\n",
    "    inputs = tf.layers.batch_normalization(inputs, training=is_training)\n",
    "    return tf.nn.relu(inputs)\n",
    "\n",
    "\n",
    "def conv2d(inputs, filters, is_training, kernel_size=3, stride=1, reg=1e-4, data_format='channels_last'):\n",
    "    inputs = tf.layers.conv2d(inputs, filters, kernel_size, strides=stride, padding=\"same\", kernel_initializer=initializer,\n",
    "                              kernel_regularizer=tf.contrib.layers.l2_regularizer(\n",
    "                                  scale=reg),\n",
    "                              bias_regularizer=tf.contrib.layers.l2_regularizer(\n",
    "                                  scale=reg),\n",
    "                              data_format=data_format)\n",
    "    return inputs\n",
    "\n",
    "# Resnet unit\n",
    "\n",
    "\n",
    "def ResNet_unit(inputs, filters, is_training, i, j, subsample=False, reg=1e-4, final_unit=False, data_format='channels_last'):\n",
    "    with tf.variable_scope(f\"conv{i+2}_{j+1}\"):\n",
    "        shortcut = inputs\n",
    "        stride = 2 if subsample else 1\n",
    "\n",
    "        # for the first unit batch_norm_relu before splitting into two paths\n",
    "        if i == 0 and j == 0:\n",
    "            inputs = batch_norm_relu(inputs, is_training)\n",
    "            shortcut = inputs\n",
    "            inputs = conv2d(inputs, filters, is_training,\n",
    "                            stride=stride, reg=reg, data_format=data_format)\n",
    "        else:\n",
    "            inputs = batch_norm_relu_conv2d(\n",
    "                inputs, filters, is_training, stride=stride, reg=reg, data_format=data_format)\n",
    "        inputs = batch_norm_relu_conv2d(\n",
    "            inputs, filters, is_training, reg=reg, data_format=data_format)\n",
    "\n",
    "        if subsample:\n",
    "            if data_format == 'channels_last':\n",
    "                paddings = tf.constant(\n",
    "                    [[0, 0], [0, 0], [0, 0], [0, filters // 2]])\n",
    "                # reduce image height and width by striding as in resnet paper\n",
    "                shortcut = shortcut[:, ::2, ::2, :]\n",
    "            else:\n",
    "                paddings = tf.constant(\n",
    "                    [[0, 0], [0, filters // 2], [0, 0], [0, 0]])\n",
    "                shortcut = shortcut[:, :, ::2, ::2]\n",
    "            shortcut = tf.pad(shortcut, paddings)\n",
    "        inputs = shortcut + inputs\n",
    "\n",
    "        # Final activation\n",
    "        if final_unit:\n",
    "            inputs = batch_norm_relu(inputs, is_training)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "\n",
    "def model_ResNetv2(inputs, is_training, total_layers=20, num_classes=10, reg=2e-4, data_format='channels_last'):\n",
    "    \"\"\"\n",
    "    Creates a ResNetv2 model based upon CIFAR-10 ResNet.  \n",
    "    Total_layers = 6n + 2\n",
    "    \"\"\"\n",
    "    assert (total_layers - 2) % 6 == 0\n",
    "    num_layers = (total_layers - 2) // 6\n",
    "    filters = [16, 32, 64]\n",
    "\n",
    "    # first do a single convolution ResNet_unit with no addition\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        inputs = conv2d(\n",
    "            inputs, filters[0], is_training, stride=2, reg=reg, data_format=data_format)\n",
    "\n",
    "    # now some ResNet units\n",
    "    for i in range(3):\n",
    "        for j in range(num_layers):\n",
    "            # don't subsample on first go round\n",
    "            subsample = i > 0 and j == 0\n",
    "            final = i == 2 and j == num_layers-1\n",
    "            inputs = ResNet_unit(inputs, filters[i], is_training, i, j,\n",
    "                                 subsample=subsample, reg=reg, final_unit=final, data_format=data_format)\n",
    "\n",
    "    # Global average pooling, 10 way FC layer and then output to scores.\n",
    "    # Global average pooling is same as doing reduce_mean\n",
    "    if data_format == 'channels_last':\n",
    "        reduce_axis = [1, 2]\n",
    "    else:\n",
    "        reduce_axis = [2, 3]\n",
    "    inputs = tf.reduce_mean(inputs, axis=reduce_axis)\n",
    "    inputs = tf.layers.flatten(inputs)\n",
    "    scores = tf.layers.dense(inputs, num_classes, kernel_initializer=initializer,\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(\n",
    "                                 scale=reg),\n",
    "                             bias_regularizer=tf.contrib.layers.l2_regularizer(scale=reg))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small test to check that our neutral network works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_ResNet_fc():\n",
    "    \"\"\" A small unit test for model_ResNetv2 above. \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    x = tf.zeros((50, 3, 18, 11))\n",
    "    scores = model_ResNetv2(x, 1, num_classes=num_classes,\n",
    "                            data_format='channels_first')\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        scores_np = sess.run(scores)\n",
    "        print(scores_np.shape)\n",
    "\n",
    "# test_model_ResNet_fc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_earth_mover(labels, logits):\n",
    "    \"\"\"\n",
    "    Computes the normalised squared Earth Moverâ€™s Distance loss from https://arxiv.org/pdf/1611.05916.pdf.\n",
    "    Since our classes are ordered this loss behaves much better than the usual softmax cross entropy.\n",
    "\n",
    "    Inputs:\n",
    "    - labels: Tensor of shape [batch_size] and dtype int32 or int64.\n",
    "      Each entry in labels must be an index in [0, num_classes)\n",
    "    - logits: Unscaled log probabilities of shape [batch_size, num_classes]\n",
    "\n",
    "    Returns:\n",
    "    - loss: A Tensor of the same shape as labels and of the same type as logits with the softmax cross entropy loss.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope(\"sparse_earth_mover\"):\n",
    "        num_classes = tf.shape(logits)[-1]\n",
    "        batch_size = tf.shape(labels)[0]\n",
    "\n",
    "        logits_norm = tf.nn.softmax(logits)\n",
    "        one_hot_labels = tf.one_hot(labels, num_classes)\n",
    "\n",
    "        cdf_labels = tf.cumsum(one_hot_labels, axis=-1)\n",
    "        cdf_logits = tf.cumsum(logits_norm, axis=-1)\n",
    "        loss = tf.sqrt(tf.reduce_mean(\n",
    "            tf.square(cdf_labels - cdf_logits), axis=-1))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_acc_tb(sess, x, next_element, scores, is_training, FLAG_print=True, grades_away=0):\n",
    "    \"\"\"\n",
    "    Checks the accuracy of a classification model.\n",
    "\n",
    "    Inputs:\n",
    "    - sess: A TensorFlow Session that will be used to run the graph\n",
    "    - x: A TensorFlow placeholder Tensor where input images should be fed\n",
    "    - next_element: A TensorFlow placeholder Tensor where the next batch of elements will be fed\n",
    "    - scores: A TensorFlow Tensor representing the scores output from the\n",
    "      model; this is the Tensor we will ask TensorFlow to evaluate.\n",
    "    - A TensorFlow placeholder Tensor where a bool should be fed if we are training the dataset\n",
    "    - grades_away: An int.  Accuracy is calculated within grades_away\n",
    "    - FLAG_print: a bool to decide if we print how accurate we are\n",
    "\n",
    "    Returns: Accuracy of the model\n",
    "    \"\"\"\n",
    "    num_correct, num_samples = 0, 0\n",
    "    with tf.name_scope('accuracy'):\n",
    "        while True:\n",
    "            try:\n",
    "                (x_np, y_np) = sess.run(next_element)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "            feed_dict = {x: x_np, is_training: False}\n",
    "            scores_np = sess.run(scores, feed_dict=feed_dict)\n",
    "            y_pred = scores_np.argmax(axis=1)\n",
    "            num_samples += x_np.shape[0]\n",
    "            num_correct += np.sum(np.abs(y_pred - y_np) <= grades_away)\n",
    "        assert num_samples > 0\n",
    "        acc = float(num_correct) / num_samples\n",
    "        if FLAG_print == True:\n",
    "            print('Got %d / %d correct (%.2f%%)' %\n",
    "                  (num_correct, num_samples, 100 * acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_acc_train(x, y, scores, grades_away=0):\n",
    "    \"\"\"\n",
    "    Check accuracy on a classification model from a batch of data.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A TensorFlow placeholder Tensor where input images should be fed\n",
    "    - y: A TensorFlow placeholder Tensor where input classification scores\n",
    "      should be fed\n",
    "    - scores: A TensorFlow Tensor representing the scores output from the\n",
    "      model; this is the Tensor we will ask TensorFlow to evaluate.\n",
    "    - grades_away: An int.  Accuracy is calculated within grades_away\n",
    "\n",
    "    Returns: Accuracy of the model on a batch of training data\n",
    "    \"\"\"\n",
    "    num_correct, num_samples = 0, 0\n",
    "    with tf.name_scope('accuracy'):\n",
    "        y_pred = tf.argmax(scores, axis=1, output_type=tf.int32)\n",
    "        bool_acc = tf.abs(y_pred - y) <= grades_away\n",
    "        acc = tf.cast(tf.count_nonzero(bool_acc), tf.float32) / \\\n",
    "            tf.cast(tf.shape(x)[0], tf.float32)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_init_fn, optimizer_init_fn, lr, num_epochs=1, experiment_name=\"\",\n",
    "          grades_away=1, cross_coeff=0, decay_at=[], decay_to=[],\n",
    "          save=False, log=True, save_graph=False, val=True):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.layers. It trains\n",
    "    a model for num_epochs, peridoically checks the accuracy on the validation\n",
    "    dataset, logs the training data to Tensorboard, saves the graph, and tests \n",
    "    the final accuracy on the test dataset.\n",
    "\n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    - data_format: Channels first or last for the tensors\n",
    "    - grades_away: An int.  Accuracy is calculated within grades_away\n",
    "    - experiment_nume: The name to call the experiement when logging and saving\n",
    "    - cross_coeff: A coefficient infront of the cross entropy loss.\n",
    "    - deacy_at: A list of epochs to decay the learning rate at\n",
    "    - decay_to: A list of learning rates to decay to\n",
    "    - save: A bool to decide if we save the Tensorflow graph after training the model\n",
    "    - log: A bool to decide to log the training for Tensorboard\n",
    "    - save_graph: A bool to decide if we save the computational graph for Tensorboard\n",
    "    - val: A bool to decide if we check the accuracy on the validation data.\n",
    "      Set to False if there is no validation dataset.\n",
    "\n",
    "    Returns:\n",
    "    - acc_val: Accuracy on the validation dataset.  This is np.nan if val=False\n",
    "    - acc_test: Accuracy on the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # construct the datasets\n",
    "    (next_element_train, next_element_test, train_init_op,\n",
    "     val_init_op, test_init_op, steps_to_epochs) \\\n",
    "        = construct_datasets(num_epochs)\n",
    "    (x, y) = next_element_train\n",
    "\n",
    "    # declare placeholders\n",
    "    is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "    lr_var = tf.Variable(lr, trainable=False, name='learning_rate')\n",
    "\n",
    "    # Whenever we need to record the loss, feed the mean test accuracy to this placeholder\n",
    "    with tf.name_scope('acc'):\n",
    "        tf_acc_ph = tf.placeholder(tf.float32, shape=None, name='acc_summary')\n",
    "        # Create a scalar summary object for the accuracy so it can be displayed\n",
    "        tf.summary.scalar(f\"accuracy_within_{grades_away}_grades\", tf_acc_ph)\n",
    "\n",
    "    # Use the model function to build the forward pass.\n",
    "    scores = model_init_fn(x, is_training)\n",
    "\n",
    "    # Compute the losses\n",
    "    if cross_coeff != 0:\n",
    "        loss_cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=y, logits=scores)\n",
    "        loss_cross_entropy = cross_coeff*tf.reduce_mean(loss_cross_entropy)\n",
    "    else:\n",
    "        loss_cross_entropy = 0\n",
    "    loss_em = sparse_earth_mover(labels=y, logits=scores)\n",
    "    loss_em = tf.reduce_mean(loss_em)\n",
    "    loss_reg = tf.losses.get_regularization_loss()\n",
    "    loss = loss_em + loss_reg + loss_cross_entropy\n",
    "\n",
    "    # Tensorboard logging scalars\n",
    "    if cross_coeff != 0:\n",
    "        tf.summary.scalar('loss_cross_entropy', loss_cross_entropy)\n",
    "    tf.summary.scalar('loss_em', loss_em)\n",
    "    tf.summary.scalar('loss_reg', loss_reg)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('learning_rate', lr_var)\n",
    "\n",
    "    # initialise the optimizer and create the training operation\n",
    "    optimizer = optimizer_init_fn(lr_var)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        with tf.name_scope('train'):\n",
    "            train_op = optimizer.minimize(loss)\n",
    "\n",
    "    # check train accuarcy function\n",
    "    acc_train_op = check_acc_train(x, y, scores, grades_away=grades_away)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Tensorboard, merge all summaries but the error ones\n",
    "        merged = tf.summary.merge_all(scope=\"(?!acc)\")\n",
    "        merged_acc = tf.summary.merge_all(scope=\"(acc)\")\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Create the saver and Tensorboard log writers\n",
    "        if save:\n",
    "            saver = tf.train.Saver()\n",
    "        if log:\n",
    "            log_path = \"C:/tmp/logs\"\n",
    "            if save_graph:\n",
    "                train_writer = tf.summary.FileWriter(\n",
    "                    log_path + '/train/' + experiment_name, sess.graph)\n",
    "            else:\n",
    "                train_writer = tf.summary.FileWriter(\n",
    "                    log_path + '/train/' + experiment_name)\n",
    "            test_writer = tf.summary.FileWriter(\n",
    "                log_path + '/test/' + experiment_name)\n",
    "\n",
    "        # Initialize an iterator over the training dataset.\n",
    "        sess.run(train_init_op)\n",
    "        t = 0\n",
    "        while True:\n",
    "            # decay learning rate\n",
    "            if (t / steps_to_epochs) in decay_at:\n",
    "                lr_var.load(\n",
    "                    decay_to[decay_at.index(t / steps_to_epochs)], sess)\n",
    "\n",
    "            # train on next batch of data\n",
    "            feed_dict = {is_training: True}\n",
    "            try:\n",
    "                # check running accuracy on training batch and add to tensorboard every 20 steps\n",
    "                if (t + 1) % 20 == 0:\n",
    "                    summary, _, acc_train = sess.run(\n",
    "                        [merged, train_op, acc_train_op], feed_dict=feed_dict)\n",
    "                    if log:\n",
    "                        train_writer.add_summary(summary, t)\n",
    "                        train_writer.add_summary(\n",
    "                            sess.run(merged_acc, feed_dict={tf_acc_ph: acc_train}), t)\n",
    "                else:\n",
    "                    # train normally\n",
    "                    loss_np, _ = sess.run(\n",
    "                        [loss, train_op], feed_dict=feed_dict)\n",
    "                    # stop training if loss blows up\n",
    "                    if np.isnan(loss_np):\n",
    "                        if val:\n",
    "                            # roughly expected accuracy from random guess\n",
    "                            return (3/15, 3/15)\n",
    "                        else:\n",
    "                            break\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "            t += 1\n",
    "\n",
    "            # Check accuacry on validation dataset every epoch\n",
    "            if t % steps_to_epochs == 0 and log and val:\n",
    "                sess.run(val_init_op)\n",
    "                acc_val = check_acc_tb(sess, x, next_element_test, scores, is_training,\n",
    "                                       FLAG_print=False, grades_away=grades_away)\n",
    "                test_writer.add_summary(\n",
    "                    sess.run(merged_acc, feed_dict={tf_acc_ph: acc_val}), t)\n",
    "\n",
    "        # End of training.  Calculate accuracy on validation dataset\n",
    "        if val:\n",
    "            sess.run(val_init_op)\n",
    "            acc_val = check_acc_tb(sess, x, next_element_test, scores, is_training,\n",
    "                                   FLAG_print=False, grades_away=grades_away)\n",
    "            if log:\n",
    "                test_writer.add_summary(\n",
    "                    sess.run(merged_acc, feed_dict={tf_acc_ph: acc_val}), t)\n",
    "        else:\n",
    "            acc_val = np.NaN\n",
    "\n",
    "        print('End of training')\n",
    "        if val:\n",
    "            print(f\"Validation accuracy is {acc_val}\")\n",
    "\n",
    "        # Calculate accuracy on test dataset\n",
    "        sess.run(test_init_op)\n",
    "        acc_test = check_acc_tb(\n",
    "            sess, x, next_element_test, scores, is_training, grades_away=grades_away)\n",
    "        print(f\"Accuracy on the test dataset is {acc_test}\")\n",
    "\n",
    "        # Save the graph to disk.\n",
    "        if save:\n",
    "            save_path = saver.save(sess, f\"C:/tmp/save/{experiment_name}.ckpt\")\n",
    "        return acc_val, acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we optimiser our hyperparameters we check the model with some sensible parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of training\n",
      "Validation accuracy is 0.545123062898815\n",
      "Got 554 / 1097 correct (50.50%)\n",
      "Accuracy on the test dataset is 0.5050136736554239\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "total_layers = 14\n",
    "learning_rate = 0.005\n",
    "reg = 0.002\n",
    "cross_coeff = 0.1\n",
    "\n",
    "name = (f\"climbing_ResNet{total_layers}_lr{learning_rate}\"\n",
    "        f\"_reg{reg}_cross{cross_coeff}_momentum\")\n",
    "\n",
    "\n",
    "def model_init_fn(inputs, is_training, total_layers=total_layers, reg=reg):\n",
    "    return model_ResNetv2(inputs, is_training, total_layers=total_layers, reg=reg,\n",
    "                          num_classes=num_classes, data_format=data_format)\n",
    "\n",
    "\n",
    "def optimizer_init_fn(lr):\n",
    "    return tf.train.MomentumOptimizer(lr, momentum=0.9)\n",
    "\n",
    "\n",
    "train(model_init_fn, optimizer_init_fn, learning_rate, num_epochs=num_epochs,\n",
    "      experiment_name=name, cross_coeff=cross_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimise the hyperparameters\n",
    "We use `scikit-optimize` to perform a random grid search and Gaussian process optimisation to find the best hyperparameters: `learning rate`, `reg` and `cross_coeff`.  We train a 14 layer network over 150 epochs.  This optimisation process takes 60 minutes on my laptop.  We choose to use a Momentum optimizer as it tends to give better test performance than an Adam one.  For the learning rate we follow a process similar to the ResNet papers: we divide the learning rate by 5 at 60% of the way through the training, and we also 'warm up' the model for 10 epochs at `learning_rate / 5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 150\n",
    "total_layers = 14\n",
    "num_calls = 30\n",
    "\n",
    "# learning_rate decay similar to ResNet paper: intial warm up, then divide learning rate by 10 at 50% and 75%\n",
    "\n",
    "dim_learning_rate = Real(low=1e-4, high=1e-1, prior='log-uniform',\n",
    "                         name='learning_rate')\n",
    "dim_reg = Real(low=1e-5, high=1e0, prior='log-uniform',\n",
    "               name='reg')\n",
    "dim_cross_coeff = Real(low=1e-4, high=1e0, prior='log-uniform',\n",
    "                       name='cross_coeff')\n",
    "dimensions = [dim_learning_rate, dim_reg, dim_cross_coeff]\n",
    "checkpoint_path = f\"C:/tmp/gp_search_checkpoint_epochs_{num_epochs}.pkl\"\n",
    "checkpoint_saver = CheckpointSaver(checkpoint_path, compress=9)\n",
    "\n",
    "\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def op_acc(learning_rate, reg, cross_coeff):\n",
    "    # setup decay scheme\n",
    "    decay_at = [10, int(num_epochs * 0.6)]\n",
    "    decay_to = [learning_rate, learning_rate / 5]\n",
    "    name = (f\"climbing_ResNet{total_layers}_lr{learning_rate}_reg{reg}\"\n",
    "            f\"_cross{cross_coeff}_momentum_decay_gp_search\")\n",
    "    learning_rate = learning_rate / 5\n",
    "\n",
    "    def model_init_fn(inputs, is_training, total_layers=total_layers, reg=reg):\n",
    "        return model_ResNetv2(inputs, is_training, total_layers=total_layers,\n",
    "                              reg=reg, num_classes=num_classes, data_format=data_format)\n",
    "\n",
    "    def optimizer_init_fn(lr):\n",
    "        return tf.train.MomentumOptimizer(lr, momentum=0.9)\n",
    "\n",
    "    # Stop printing from train function\n",
    "    # (https://stackoverflow.com/questions/23610585/ipython-notebook-avoid-printing-within-a-function/23611571#23611571)\n",
    "    with io.capture_output() as captured:\n",
    "        acc_val, _ = train(model_init_fn, optimizer_init_fn, learning_rate, num_epochs=num_epochs,\n",
    "                           experiment_name=name, cross_coeff=cross_coeff,\n",
    "                           log=True, decay_at=decay_at, decay_to=decay_to)\n",
    "    return -acc_val\n",
    "\n",
    "\n",
    "x0 = [0.005, 0.002, 0.1]\n",
    "search_result = gp_minimize(func=op_acc, dimensions=dimensions, n_calls=num_calls,\n",
    "                            callback=[checkpoint_saver], verbose=True, x0=x0)\n",
    "plot_convergence(search_result)\n",
    "print(search_result.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot of how the validation accuracy increased during our hyperparameter optimisation\n",
    "\n",
    "![Convergence plot](convergence_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the model with the validation data included in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of training\n",
      "Got 676 / 1097 correct (61.62%)\n",
      "Accuracy on the test dataset is 0.6162260711030082\n"
     ]
    }
   ],
   "source": [
    "[learning_rate, reg, cross_coeff] = search_result.x\n",
    "\n",
    "# add the validation data to the training set.  Keep the test dataset untouched\n",
    "X_train = np.concatenate((X_train, X_val))\n",
    "y_train = np.concatenate((y_train, y_val))\n",
    "\n",
    "# setup decay scheme\n",
    "decay_at = [10, int(num_epochs * 0.6)]\n",
    "decay_to = [learning_rate, learning_rate / 5]\n",
    "name = (f\"climbing_ResNet{total_layers}_lr{learning_rate}_reg{reg}\"\n",
    "        f\"_cross{cross_coeff}_momentum_decay\")\n",
    "learning_rate = learning_rate / 5\n",
    "\n",
    "\n",
    "def model_init_fn(inputs, is_training, total_layers=total_layers, reg=reg):\n",
    "    return model_ResNetv2(inputs, is_training, total_layers=total_layers,\n",
    "                          reg=reg, num_classes=num_classes, data_format=data_format)\n",
    "\n",
    "\n",
    "def optimizer_init_fn(lr):\n",
    "    return tf.train.MomentumOptimizer(lr, momentum=0.9)\n",
    "\n",
    "\n",
    "_, acc_test = train(model_init_fn, optimizer_init_fn, learning_rate, num_epochs=num_epochs,\n",
    "                    experiment_name=name, cross_coeff=cross_coeff,\n",
    "                    decay_at=decay_at, decay_to=decay_to, val=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
