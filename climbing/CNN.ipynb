{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we train a convolution neural network (based upon ResNetv2) to classify routes by their grades and achieve 70% accuracy on the test dataset (using a one out accuracy, the true grade can be +- 1 of our guess).  We experiment with three different loss functions to try and take advantage of the ordering of our labels (grades arranged on a number line).  This allows us to gain 70% accuracy (one out) using the CJS (cummlative Jensen-Shannon divergence) as our loss function compared to 65% accuracy using the standard cross-entropy loss.  For more details about the CJS loss see https://arxiv.org/pdf/1708.07089.pdf.  We also test the squared earth mover's distance (or Wasserstein metric) as a loss function but this gives worse results (60%) - https://arxiv.org/pdf/1611.05916.pdf.\n",
    "\n",
    "### Building the data pipeline\n",
    "We load the input data, create the training, validation and testing datasets ensuring the proper distribution of grade 6, 7 and 8's in each, and then build the data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.utils import io\n",
    "\n",
    "from skopt import gp_minimize, callbacks, load\n",
    "from skopt.space import Real\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.callbacks import CheckpointSaver\n",
    "# Need skopt.__version__ > 0.5.2 or pip install git+https://github.com/scikit-optimize/scikit-optimize/\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rcParams[\"image.origin\"] = 'lower'\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (2575, 3, 18, 11) float32\n",
      "Train labels shape:  (2575,) int32\n",
      "Validation data shape:  (733, 3, 18, 11)\n",
      "Validation labels shape:  (733,)\n",
      "Test data shape:  (366, 3, 18, 11)\n",
      "Test labels shape:  (366,)\n"
     ]
    }
   ],
   "source": [
    "def load_data(train=0.7, val=0.2, test=0.1, data_format='channels_first'):\n",
    "    \"\"\"\n",
    "    Loads the datasets from data/data.npz and randomly creates the train, test and \n",
    "    validation datasets.\n",
    "\n",
    "    Inputs:\n",
    "    - train, val, test: the fraction of the dataset in the train dataset, validation dataset\n",
    "      and test dataset respectively\n",
    "    - data_format: A string, one of 'channels_last' (default) or 'channels_first'.\n",
    "      'channels_last' best for CPU and 'channels_first' best for GPU\n",
    "    Returns:\n",
    "    - 6 numpy arrays: X_train, y_train, X_val, y_val, X_test, y_test in either\n",
    "      channel_last or channel_first format\n",
    "    - grade_dict : a dictionary of coverting the grades to numerical scores.\n",
    "    \"\"\"\n",
    "    # check fraction of datasets sum up to 1 (ignoring float rounding errors)\n",
    "    assert np.isclose(train + val + test, 1)\n",
    "\n",
    "    # load the data, n.b. arrays are sorted by grade\n",
    "    loaded = np.load('data/data_user.npz')\n",
    "    moves = loaded['moves']\n",
    "    grades = loaded['grades']\n",
    "    grade_dict = loaded['grade_dict'][()]\n",
    "\n",
    "    # Find partition arguments between grade 6, 7 & 8\n",
    "    part_arg = np.searchsorted(grades, [grade_dict['7A'], grade_dict['8A']])\n",
    "\n",
    "    # now shuffle within the grade 6's, 7's and 8's\n",
    "    permute_idx = np.arange(grades.shape[0])\n",
    "    np.random.shuffle(permute_idx[:part_arg[0]])\n",
    "    np.random.shuffle(permute_idx[part_arg[0]:part_arg[1]])\n",
    "    np.random.shuffle(permute_idx[part_arg[1]:])\n",
    "    moves = moves[permute_idx]\n",
    "    grades = grades[permute_idx]\n",
    "\n",
    "    # data processing\n",
    "    if data_format == 'channels_first':\n",
    "        moves = np.moveaxis(moves, -1, 1)\n",
    "    moves = moves.astype(np.float32)\n",
    "\n",
    "    # create the train, val and test datasets from the grade classes\n",
    "    part_start = np.append(0, part_arg)\n",
    "    size = np.array([part_arg[0], part_arg[1] - part_arg[0],\n",
    "                     len(grades) - part_arg[1]])\n",
    "\n",
    "    num_val = (val * size).astype(int)\n",
    "    num_test = (test * size).astype(int)\n",
    "    num_train = (size - num_val - num_test).astype(int)\n",
    "\n",
    "    # generate the training, val and test sets\n",
    "    slice_range = [part_start,\n",
    "                   part_start + num_train,\n",
    "                   part_start + num_train + num_val,\n",
    "                   part_start + num_train + num_val + num_test]\n",
    "    X, y = [], []\n",
    "    for j in range(3):\n",
    "        grade_list, moves_list = [], []\n",
    "        for i in range(3):\n",
    "            grade_list.append(grades[slice_range[j][i]: slice_range[j+1][i]])\n",
    "            moves_list.append(moves[slice_range[j][i]: slice_range[j+1][i]])\n",
    "        X.append(np.concatenate(moves_list))\n",
    "        y.append(np.concatenate(grade_list))\n",
    "\n",
    "    X_train, X_val, X_test = X\n",
    "    y_train, y_val, y_test = y\n",
    "\n",
    "    # check: sets are the correct length\n",
    "    assert (len(y_val) == np.sum(num_val) and len(y_test) == np.sum(num_test)\n",
    "            and len(y_train) == np.sum(num_train))\n",
    "\n",
    "    # Normalize the data: subtract the mean pixel and divide by std\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, grade_dict\n",
    "\n",
    "\n",
    "data_format = 'channels_first'\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, grade_dict = load_data()\n",
    "num_classes = len(grade_dict) - 1\n",
    "\n",
    "print('Train data shape: ', X_train.shape, X_train.dtype)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "\n",
    "def construct_datasets(num_epochs=1):\n",
    "    \"\"\"\n",
    "    Constructs the datasets in Tensorflow.\n",
    "\n",
    "    Inputs: \n",
    "    - num_epochs: The number of epochs to run the training data for\n",
    "\n",
    "    Outputs:\n",
    "    - next_element_train, next_element_test: get_next() method for train and test dataset iterators.\n",
    "      The next_element_test is either from the validation or testing dataset depending on which has\n",
    "      been initialised\n",
    "    - train_init_op, val_init_op, test_init_op:\n",
    "      iterator initialisation operations for the respective datasets\n",
    "    - steps_to_epochs: The number of integers steps to each epoch of the training dataset\n",
    "    \"\"\"\n",
    "    prefetch = 2\n",
    "\n",
    "    # make sure the dataset is on the CPU to leave the GPU for training the model\n",
    "    with tf.device('/cpu:0'):\n",
    "        with tf.variable_scope('train_dataset'):\n",
    "            dataset_train = tf.data.Dataset.from_tensor_slices(\n",
    "                (X_train, y_train))\n",
    "            dataset_train = dataset_train.apply(\n",
    "                tf.data.experimental.shuffle_and_repeat(len(X_train), count=num_epochs))\n",
    "            dataset_train = dataset_train.shuffle(len(X_train))\n",
    "            dataset_train = dataset_train.batch(batch_size).prefetch(prefetch)\n",
    "\n",
    "        with tf.variable_scope('validation_dataset'):\n",
    "            dataset_val = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "            dataset_val = dataset_val.batch(batch_size).prefetch(prefetch)\n",
    "        with tf.variable_scope('test_dataset'):\n",
    "            dataset_test = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "            dataset_test = dataset_test.batch(batch_size).prefetch(prefetch)\n",
    "\n",
    "        iterator_train = tf.data.Iterator.from_structure(dataset_train.output_types,\n",
    "                                                         dataset_train.output_shapes)\n",
    "        next_element_train = iterator_train.get_next()\n",
    "        iterator_test = tf.data.Iterator.from_structure(dataset_train.output_types,\n",
    "                                                        dataset_train.output_shapes)\n",
    "        next_element_test = iterator_test.get_next()\n",
    "\n",
    "        train_init_op = iterator_train.make_initializer(dataset_train)\n",
    "        val_init_op = iterator_test.make_initializer(dataset_val)\n",
    "        test_init_op = iterator_test.make_initializer(dataset_test)\n",
    "        steps_to_epochs = len(X_train) // batch_size\n",
    "\n",
    "    return next_element_train, next_element_test, train_init_op, val_init_op, test_init_op, steps_to_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the neural network\n",
    "\n",
    "Our neural network is a deep network based upon Resnetv2 and has the same structure as the CIFAR-10 version of ResNetv2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "# Helper layer functions\n",
    "\n",
    "\n",
    "def batch_norm_relu_conv2d(inputs, filters, is_training, stride=1, reg=1e-4, drop_rate=0, data_format='channels_first'):\n",
    "    inputs = batch_norm_relu(inputs, is_training)\n",
    "    inputs = conv2d(inputs, filters, is_training, stride=stride,\n",
    "                    reg=reg, drop_rate=drop_rate, data_format=data_format)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def batch_norm_relu(inputs, is_training):\n",
    "    inputs = tf.layers.batch_normalization(inputs, training=is_training)\n",
    "    return tf.nn.relu(inputs)\n",
    "\n",
    "\n",
    "def conv2d(inputs, filters, is_training, kernel_size=3, stride=1, reg=1e-4, drop_rate=0, data_format='channels_first'):\n",
    "    inputs = tf.layers.conv2d(inputs, filters, kernel_size, strides=stride, padding=\"same\", kernel_initializer=initializer,\n",
    "                              kernel_regularizer=tf.contrib.layers.l2_regularizer(\n",
    "                                  scale=reg),\n",
    "                              data_format=data_format)\n",
    "    if drop_rate != 0:\n",
    "        # n.b rate = 1 - keep_prob\n",
    "        inputs = tf.layers.dropout(\n",
    "            inputs, rate=drop_rate, training=is_training)\n",
    "    return inputs\n",
    "\n",
    "# Resnet unit\n",
    "\n",
    "\n",
    "def ResNet_unit(inputs, filters, is_training, i, j, subsample=False, reg=1e-4,\n",
    "                final_unit=False, data_format='channels_first', drop_rate=0):\n",
    "    with tf.variable_scope(f\"conv{i+2}_{j+1}\"):\n",
    "        shortcut = inputs\n",
    "        stride = 2 if subsample else 1\n",
    "\n",
    "        # for the first unit batch_norm_relu before splitting into two paths\n",
    "        if i == 0 and j == 0:\n",
    "            inputs = batch_norm_relu(inputs, is_training)\n",
    "            shortcut = inputs\n",
    "            inputs = conv2d(inputs, filters, is_training, drop_rate=drop_rate,\n",
    "                            stride=stride, reg=reg, data_format=data_format)\n",
    "        else:\n",
    "            inputs = batch_norm_relu_conv2d(\n",
    "                inputs, filters, is_training, stride=stride, reg=reg,\n",
    "                data_format=data_format, drop_rate=drop_rate)\n",
    "        inputs = batch_norm_relu_conv2d(\n",
    "            inputs, filters, is_training, reg=reg, drop_rate=drop_rate, data_format=data_format)\n",
    "\n",
    "        if subsample:\n",
    "            if data_format == 'channels_last':\n",
    "                paddings = tf.constant(\n",
    "                    [[0, 0], [0, 0], [0, 0], [0, filters // 2]])\n",
    "                # reduce image height and width by striding as in resnet paper\n",
    "                shortcut = shortcut[:, ::2, ::2, :]\n",
    "            else:\n",
    "                paddings = tf.constant(\n",
    "                    [[0, 0], [0, filters // 2], [0, 0], [0, 0]])\n",
    "                shortcut = shortcut[:, :, ::2, ::2]\n",
    "            shortcut = tf.pad(shortcut, paddings)\n",
    "        inputs = shortcut + inputs\n",
    "\n",
    "        # Final activation\n",
    "        if final_unit:\n",
    "            inputs = batch_norm_relu(inputs, is_training)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "\n",
    "def model_ResNetv2(inputs, is_training, total_layers=20, num_classes=10, reg=2e-4,\n",
    "                   drop_rate=0.5, data_format='channels_first'):\n",
    "    \"\"\"\n",
    "    Creates a ResNetv2 model based upon CIFAR-10 ResNet.  \n",
    "    Total_layers = 6n + 2\n",
    "    \"\"\"\n",
    "    assert (total_layers - 2) % 6 == 0\n",
    "    num_layers = (total_layers - 2) // 6\n",
    "    filters = [16, 32, 64]\n",
    "\n",
    "    # first do a single convolution ResNet_unit with no addition\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        inputs = conv2d(\n",
    "            inputs, filters[0], is_training, stride=2, reg=reg,\n",
    "            drop_rate=drop_rate, data_format=data_format)\n",
    "\n",
    "    # now some ResNet units\n",
    "    for i in range(3):\n",
    "        for j in range(num_layers):\n",
    "            # don't subsample on first go round\n",
    "            subsample = i > 0 and j == 0\n",
    "            final = i == 2 and j == num_layers-1\n",
    "            inputs = ResNet_unit(inputs, filters[i], is_training, i, j,\n",
    "                                 subsample=subsample, reg=reg, final_unit=final, data_format=data_format)\n",
    "\n",
    "    # Global average pooling, 10 way FC layer and then output to scores.\n",
    "    # Global average pooling is same as doing reduce_mean\n",
    "    if data_format == 'channels_last':\n",
    "        reduce_axis = [1, 2]\n",
    "    else:\n",
    "        reduce_axis = [2, 3]\n",
    "    inputs = tf.reduce_mean(inputs, axis=reduce_axis)\n",
    "    inputs = tf.layers.flatten(inputs)\n",
    "    scores = tf.layers.dense(inputs, num_classes, kernel_initializer=initializer,\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(\n",
    "                                 scale=reg))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small test to check that our neutral network works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_ResNet_fc():\n",
    "    \"\"\" A small unit test for model_ResNetv2 above. \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    x = tf.zeros((50, 3, 18, 11))\n",
    "    scores = model_ResNetv2(x, 1, num_classes=num_classes, drop_rate=0.8,\n",
    "                            data_format='channels_first')\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        scores_np = sess.run(scores)\n",
    "        print(scores_np.shape)\n",
    "\n",
    "# test_model_ResNet_fc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_earth_mover(labels, logits, p=2):\n",
    "    \"\"\"\n",
    "    Computes the normalised squared Earth Moverâ€™s Distance loss from https://arxiv.org/pdf/1611.05916.pdf.\n",
    "    Since our classes are ordered this loss behaves much better than the usual softmax cross entropy.\n",
    "\n",
    "    Inputs:\n",
    "    - labels: Tensor of shape [batch_size] and dtype int32 or int64.\n",
    "      Each entry in labels must be an index in [0, num_classes)\n",
    "    - logits: Unscaled log probabilities of shape [batch_size, num_classes]\n",
    "    - p: which l^p norm to use. p = 2 represents the squared Earth mover's distance\n",
    "\n",
    "    Returns:\n",
    "    - loss: A Tensor of the same shape as labels and of the same type as logits with the softmax cross entropy loss.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope(\"sparse_earth_mover\"):\n",
    "        num_classes = tf.shape(logits)[-1]\n",
    "\n",
    "        logits_normed = tf.nn.softmax(logits)\n",
    "        one_hot_labels = tf.one_hot(labels, num_classes)\n",
    "\n",
    "        cdf_labels = tf.cumsum(one_hot_labels, axis=-1)\n",
    "        cdf_logits = tf.cumsum(logits_normed, axis=-1)\n",
    "        if p == 2:\n",
    "            loss = tf.sqrt(tf.reduce_mean(\n",
    "                tf.square(cdf_labels - cdf_logits), axis=-1))\n",
    "        if p == 1:\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.abs(cdf_labels - cdf_logits), axis=-1)\n",
    "        else:\n",
    "            loss = (tf.reduce_mean(\n",
    "                (cdf_labels - cdf_logits) ** p, axis=-1)) ** (1.0 / p)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CJS(labels, logits):\n",
    "    \"\"\"Computes the symmetrical discrete cumulative Jensen-Shannon divergence from https://arxiv.org/pdf/1708.07089.pdf\n",
    "\n",
    "    Inputs:\n",
    "    - labels: Tensor of shape [batch_size] and dtype int32 or int64.\n",
    "      Each entry in labels must be an index in [0, num_classes)\n",
    "    - logits: Unscaled log probabilities of shape [batch_size, num_classes]\n",
    "\n",
    "    Returns:\n",
    "    - loss: A Tensor of the same shape as labels and of the same type as logits with the softmax cross entropy loss.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"CJS_loss\"):\n",
    "        num_classes = tf.shape(logits)[-1]\n",
    "\n",
    "        logits_normed = tf.nn.softmax(logits)\n",
    "        one_hot_labels = tf.one_hot(labels, num_classes)\n",
    "\n",
    "        cdf_labels = tf.cumsum(one_hot_labels, axis=-1)\n",
    "        cdf_logits = tf.cumsum(logits_normed, axis=-1)\n",
    "\n",
    "        def ACCJS(p, q):\n",
    "            with tf.name_scope(\"ACCJS\"):\n",
    "                # if p(i) = 0 then ACCJS(p, q)(i) = 0 since xlog(x) -> 0 as x-> 0\n",
    "                p = tf.clip_by_value(p, 1e-10, 1.0)\n",
    "                return 0.5 * tf.reduce_sum(p * tf.log(p / (0.5 * (p + q))), axis=-1)\n",
    "\n",
    "        loss = ACCJS(cdf_logits, cdf_labels) + ACCJS(cdf_labels, cdf_logits)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_acc_test(sess, x, next_element, scores, is_training):\n",
    "    \"\"\"\n",
    "    Checks the accuracy of a classification model.\n",
    "\n",
    "    Inputs:\n",
    "    - sess: A TensorFlow Session that will be used to run the graph\n",
    "    - x: A TensorFlow placeholder Tensor where input images should be fed\n",
    "    - next_element: A TensorFlow placeholder Tensor where the next batch of elements will be fed\n",
    "    - scores: A TensorFlow Tensor representing the scores output from the\n",
    "      model; this is the Tensor we will ask TensorFlow to evaluate.\n",
    "    - A TensorFlow placeholder Tensor where a bool should be fed if we are training the dataset\n",
    "\n",
    "    Returns: Accuracy of the model\n",
    "    \"\"\"\n",
    "    exact, top3, one_out, num_samples = [0.0] * 4\n",
    "    with tf.name_scope('accuracy'):\n",
    "        while True:\n",
    "            try:\n",
    "                (x_np, y_np) = sess.run(next_element)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "            feed_dict = {x: x_np, is_training: False}\n",
    "            scores_np = sess.run(scores, feed_dict=feed_dict)\n",
    "            num_samples += x_np.shape[0]\n",
    "            # find top 3 and top 1 predictions (nb argpartition doesn't sort)\n",
    "            pred_top3 = np.argpartition(scores_np, -3, axis=-1)[:, -3:]\n",
    "            pred_exact = scores_np.argmax(axis=-1)\n",
    "            # add num correct\n",
    "            # add extra dimension to y_np to broadcast\n",
    "            top3 += np.sum((pred_top3 - y_np[:, None]) == 0)\n",
    "            one_out += np.sum(np.abs(pred_exact - y_np) <= 1)\n",
    "            exact += np.sum(pred_exact == y_np)\n",
    "        acc_top3 = top3 / num_samples\n",
    "        acc_one_out = one_out / num_samples\n",
    "        acc_exact = exact / num_samples\n",
    "    return acc_exact, acc_one_out, acc_top3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_acc_train(x, y, scores):\n",
    "    \"\"\"\n",
    "    Check accuracy on a classification model from a batch of data.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A TensorFlow placeholder Tensor where input images should be fed\n",
    "    - y: A TensorFlow placeholder Tensor where input classification scores\n",
    "      should be fed\n",
    "    - scores: A TensorFlow Tensor representing the scores output from the\n",
    "      model; this is the Tensor we will ask TensorFlow to evaluate.\n",
    "\n",
    "    Returns: Accuracy of the model on a batch of training data\n",
    "    \"\"\"\n",
    "    with tf.name_scope('accuracy'):\n",
    "        num_samples = tf.cast(tf.shape(x)[0], tf.float32)\n",
    "\n",
    "        top3 = tf.count_nonzero(tf.nn.in_top_k(scores, y, 3))\n",
    "        y_pred = tf.argmax(scores, axis=1, output_type=tf.int32)\n",
    "        one_out = tf.count_nonzero(tf.abs(y_pred - y) <= 1)\n",
    "        exact = tf.count_nonzero(tf.equal(y_pred, y))\n",
    "\n",
    "        # calculate accuracies\n",
    "        acc_top3 = tf.cast(top3, tf.float32) / num_samples\n",
    "        acc_one_out = tf.cast(one_out, tf.float32) / num_samples\n",
    "        acc_exact = tf.cast(exact, tf.float32) / num_samples\n",
    "    return acc_exact, acc_one_out, acc_top3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_init_fn, optimizer_init_fn, loss_fn, lr, num_epochs=1,\n",
    "          decay_at=[], decay_to=[], experiment_name=\"\",\n",
    "          save=False, log=True, save_graph=False, val=True):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.layers. It trains\n",
    "    a model for num_epochs, peridoically checks the accuracy on the validation\n",
    "    dataset, logs the training data to Tensorboard, saves the graph, and tests \n",
    "    the final accuracy on the test dataset.\n",
    "\n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - loss_fn: The loss function to use.  It takes labels and logits as arguments and\n",
    "      when called it calculates the loss.\n",
    "    - lr: The learning rate to use for the optimiser.\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    - data_format: Channels first or last for the tensors\n",
    "    - experiment_nume: The name to call the experiement when logging and saving\n",
    "    - deacy_at: A list of epochs to decay the learning rate at\n",
    "    - decay_to: A list of learning rates to decay to\n",
    "    - save: A bool to decide if we save the Tensorflow graph after training the model\n",
    "    - log: A bool to decide to log the training for Tensorboard\n",
    "    - save_graph: A bool to decide if we save the computational graph for Tensorboard\n",
    "    - val: A bool to decide if we check the accuracy on the validation data.\n",
    "      Set to False if there is no validation dataset.\n",
    "\n",
    "    Returns:\n",
    "    - acc_val: Accuracy on the validation dataset.  This is np.nan if val=False\n",
    "    - acc_test: Accuracy on the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # construct the datasets\n",
    "    (next_element_train, next_element_test, train_init_op,\n",
    "     val_init_op, test_init_op, steps_to_epochs) \\\n",
    "        = construct_datasets(num_epochs)\n",
    "    (x, y) = next_element_train\n",
    "\n",
    "    # declare placeholders\n",
    "    is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "    lr_var = tf.Variable(lr, trainable=False, name='learning_rate')\n",
    "\n",
    "    # Whenever we need to record the loss, feed the test accuracy to these placeholders\n",
    "    with tf.name_scope('acc'):\n",
    "        tf_acc_ph_1out = tf.placeholder(tf.float32, shape=None)\n",
    "        tf_acc_ph_exact = tf.placeholder(tf.float32, shape=None)\n",
    "        tf_acc_ph_top3 = tf.placeholder(tf.float32, shape=None)\n",
    "        # Create a scalar summary object for the accuracy so it can be displayed\n",
    "        tf.summary.scalar(\"accuracy_within_1\", tf_acc_ph_1out)\n",
    "        tf.summary.scalar(\"accuracy_exact\", tf_acc_ph_exact)\n",
    "        tf.summary.scalar(\"accuracy_top3\", tf_acc_ph_top3)\n",
    "\n",
    "    # Use the model function to build the forward pass.\n",
    "    scores = model_init_fn(x, is_training)\n",
    "\n",
    "    # Compute the losses\n",
    "    loss_scores = loss_fn(labels=y, logits=scores)\n",
    "    loss_scores = tf.reduce_mean(loss_scores)\n",
    "    loss_reg = tf.losses.get_regularization_loss()\n",
    "    loss = loss_scores + loss_reg\n",
    "\n",
    "    # Tensorboard logging scalars\n",
    "    tf.summary.scalar('loss_scores', loss_scores)\n",
    "    tf.summary.scalar('loss_reg', loss_reg)\n",
    "    tf.summary.scalar('total_loss', loss)\n",
    "    tf.summary.scalar('learning_rate', lr_var)\n",
    "\n",
    "    # initialise the optimizer and create the training operation\n",
    "    optimizer = optimizer_init_fn(lr_var)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        with tf.name_scope('train'):\n",
    "            train_op = optimizer.minimize(loss)\n",
    "\n",
    "    # check train accuarcy function\n",
    "    acc_train_op = check_acc_train(x, y, scores)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Tensorboard, merge all summaries but the error ones\n",
    "        merged = tf.summary.merge_all(scope=\"(?!acc)\")\n",
    "        merged_acc = tf.summary.merge_all(scope=\"(acc)\")\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Create the saver and Tensorboard log writers\n",
    "        if save:\n",
    "            saver = tf.train.Saver()\n",
    "        if log:\n",
    "            log_path = \"C:/tmp/logs\"\n",
    "            if save_graph:\n",
    "                train_writer = tf.summary.FileWriter(\n",
    "                    log_path + '/train/' + experiment_name, sess.graph)\n",
    "            else:\n",
    "                train_writer = tf.summary.FileWriter(\n",
    "                    log_path + '/train/' + experiment_name)\n",
    "            test_writer = tf.summary.FileWriter(\n",
    "                log_path + '/test/' + experiment_name)\n",
    "\n",
    "        # Initialize an iterator over the training dataset.\n",
    "        sess.run(train_init_op)\n",
    "        t = 0\n",
    "        while True:\n",
    "            # decay learning rate\n",
    "            if (t / steps_to_epochs) in decay_at:\n",
    "                lr_var.load(\n",
    "                    decay_to[decay_at.index(t / steps_to_epochs)], sess)\n",
    "\n",
    "            # train on next batch of data\n",
    "            feed_dict = {is_training: True}\n",
    "            try:\n",
    "                # check running accuracy on training batch and add to tensorboard every 20 steps\n",
    "                if (t + 1) % 20 == 0:\n",
    "                    summary, _, acc_train = sess.run(\n",
    "                        [merged, train_op, acc_train_op], feed_dict=feed_dict)\n",
    "                    if log:\n",
    "                        train_writer.add_summary(summary, t)\n",
    "                        acc_feed_dict = {tf_acc_ph_exact: acc_train[0],\n",
    "                                         tf_acc_ph_1out: acc_train[1],\n",
    "                                         tf_acc_ph_top3: acc_train[2]}\n",
    "                        train_writer.add_summary(\n",
    "                            sess.run(merged_acc, feed_dict=acc_feed_dict), t)\n",
    "                else:\n",
    "                    # train normally\n",
    "                    loss_np, _ = sess.run(\n",
    "                        [loss, train_op], feed_dict=feed_dict)\n",
    "                    # stop training if loss blows up\n",
    "                    if np.isnan(loss_np):\n",
    "                        if val:\n",
    "                            # roughly expected accuracy from random guess\n",
    "                            return 0.2 * np.ones((2, 3))\n",
    "                        else:\n",
    "                            break\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "            t += 1\n",
    "\n",
    "            # Check accuacry on validation dataset every epoch\n",
    "            if t % steps_to_epochs == 0 and log and val:\n",
    "                sess.run(val_init_op)\n",
    "                acc_val = check_acc_test(\n",
    "                    sess, x, next_element_test, scores, is_training)\n",
    "                acc_feed_dict = {tf_acc_ph_exact: acc_val[0],\n",
    "                                 tf_acc_ph_1out: acc_val[1],\n",
    "                                 tf_acc_ph_top3: acc_val[2]}\n",
    "                test_writer.add_summary(\n",
    "                    sess.run(merged_acc, feed_dict=acc_feed_dict), t)\n",
    "\n",
    "        # End of training.  Calculate accuracy on validation dataset\n",
    "        if val:\n",
    "            sess.run(val_init_op)\n",
    "            acc_val = check_acc_test(\n",
    "                sess, x, next_element_test, scores, is_training)\n",
    "            acc_feed_dict = {tf_acc_ph_exact: acc_val[0],\n",
    "                             tf_acc_ph_1out: acc_val[1],\n",
    "                             tf_acc_ph_top3: acc_val[2]}\n",
    "\n",
    "            if log:\n",
    "                test_writer.add_summary(\n",
    "                    sess.run(merged_acc, feed_dict=acc_feed_dict), t)\n",
    "        else:\n",
    "            acc_val = None\n",
    "\n",
    "        print('End of training')\n",
    "        if val:\n",
    "            print(f\"Validation accuracy is:\")\n",
    "            print(f\"Exact: {acc_val[0]}\")\n",
    "            print(f\"1 out: {acc_val[1]}\")\n",
    "            print(f\"Top 3: {acc_val[2]}\\n\")\n",
    "\n",
    "        # Calculate accuracy on test dataset\n",
    "        sess.run(test_init_op)\n",
    "        acc_test = check_acc_test(\n",
    "            sess, x, next_element_test, scores, is_training)\n",
    "        print(f\"Accuracy on the test dataset\")\n",
    "        print(f\"Exact: {acc_test[0]}\")\n",
    "        print(f\"1 out: {acc_test[1]}\")\n",
    "        print(f\"Top 3: {acc_test[2]}\")\n",
    "\n",
    "        # Save the graph to disk.\n",
    "        if save:\n",
    "            save_path = saver.save(sess, f\"C:/tmp/save/{experiment_name}.ckpt\")\n",
    "        return acc_val, acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we optimiser our hyperparameters we check the model with some sensible parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of training\n",
      "Validation accuracy is:\n",
      "Exact: 0.4311050477489768\n",
      "1 out: 0.6412005457025921\n",
      "Top 3: 0.6862210095497954\n",
      "\n",
      "Accuracy on the test dataset\n",
      "Exact: 0.3879781420765027\n",
      "1 out: 0.6338797814207651\n",
      "Top 3: 0.6857923497267759\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 250\n",
    "total_layers = 14\n",
    "learning_rate = 1e-4\n",
    "reg = 1e-4\n",
    "drop_rate = 0.5  # set this prob of the neurons to 0\n",
    "\n",
    "name = (f\"climbing_ResNet{total_layers}_lr{learning_rate}\"\n",
    "        f\"_reg{reg}_drop{drop_rate}_adam_CJS\")\n",
    "\n",
    "\n",
    "def model_init_fn(inputs, is_training, total_layers=total_layers, reg=reg):\n",
    "    return model_ResNetv2(inputs, is_training, total_layers=total_layers, reg=reg, drop_rate=drop_rate,\n",
    "                          num_classes=num_classes)\n",
    "\n",
    "\n",
    "def optimizer_init_fn(lr):\n",
    "    return tf.train.AdamOptimizer(lr)\n",
    "\n",
    "\n",
    "[acc_val, _] = train(model_init_fn, optimizer_init_fn, CJS, learning_rate,\n",
    "                     num_epochs=num_epochs, experiment_name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimise the hyperparameters\n",
    "We use `scikit-optimize` to perform a random grid search and Gaussian process optimisation to find the best hyperparameters: `learning rate` and `reg`.  We train a 14 layer network over 150 epochs.  This optimisation process takes 60 minutes on my laptop.  We choose to use a Momentum optimizer as it tends to give better test performance than an Adam one.  For the learning rate we follow a process similar to the ResNet papers: we divide the learning rate by 5 at 60% of the way through the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 250\n",
    "total_layers = 14\n",
    "num_calls = 30\n",
    "drop_rate = 0.6  # set this prob of the neurons to 0\n",
    "dim_learning_rate = Real(low=1e-5, high=1e-1, prior='log-uniform',\n",
    "                         name='learning_rate')\n",
    "dim_reg = Real(low=1e-5, high=1e-1, prior='log-uniform',\n",
    "               name='reg')\n",
    "dimensions = [dim_learning_rate, dim_reg]\n",
    "\n",
    "\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def op_acc(learning_rate, reg):\n",
    "    name = (f\"climbing_ResNet{total_layers}_lr{learning_rate}_reg{reg}\"\n",
    "            f\"_drop{drop_rate}_adam_em_gp_search\")\n",
    "\n",
    "    def model_init_fn(inputs, is_training, total_layers=total_layers, reg=reg):\n",
    "        return model_ResNetv2(inputs, is_training, total_layers=total_layers, drop_rate=drop_rate,\n",
    "                              reg=reg, num_classes=num_classes)\n",
    "\n",
    "    def optimizer_init_fn(lr):\n",
    "        return tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Stop printing from train function\n",
    "    # (https://stackoverflow.com/questions/23610585/ipython-notebook-avoid-printing-within-a-function/23611571#23611571)\n",
    "    with io.capture_output() as captured:\n",
    "        [acc_val, _] = train(model_init_fn, optimizer_init_fn, sparse_earth_mover, learning_rate,\n",
    "                             num_epochs=num_epochs, experiment_name=name)\n",
    "    # optimise one_out accuaracy\n",
    "    return -acc_val[1]\n",
    "\n",
    "\n",
    "search_result = gp_minimize(func=op_acc, dimensions=dimensions, n_calls=num_calls,\n",
    "                            verbose=True, n_restarts_optimizer=20)\n",
    "plot_convergence(search_result)\n",
    "print(search_result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=dimensions)\n",
    "def op_acc(learning_rate, reg):\n",
    "    name = (f\"climbing_ResNet{total_layers}_lr{learning_rate}_reg{reg}\"\n",
    "            f\"_drop{drop_rate}_adam_CJS_gp_search\")\n",
    "\n",
    "    def model_init_fn(inputs, is_training, total_layers=total_layers, reg=reg):\n",
    "        return model_ResNetv2(inputs, is_training, total_layers=total_layers, drop_rate=drop_rate,\n",
    "                              reg=reg, num_classes=num_classes)\n",
    "\n",
    "    def optimizer_init_fn(lr):\n",
    "        return tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Stop printing from train function\n",
    "    with io.capture_output() as captured:\n",
    "        [acc_val, _] = train(model_init_fn, optimizer_init_fn, CJS, learning_rate,\n",
    "                             num_epochs=num_epochs, experiment_name=name)\n",
    "    # optimise one_out accuaracy\n",
    "    return -acc_val[1]\n",
    "\n",
    "\n",
    "search_result = gp_minimize(func=op_acc, dimensions=dimensions, n_calls=num_calls,\n",
    "                            verbose=True, n_restarts_optimizer=20)\n",
    "plot_convergence(search_result)\n",
    "print(search_result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=dimensions)\n",
    "def op_acc(learning_rate, reg):\n",
    "    name = (f\"climbing_ResNet{total_layers}_lr{learning_rate}_reg{reg}\"\n",
    "            f\"_drop{drop_rate}_adam_cross_entropy_gp_search\")\n",
    "\n",
    "    def model_init_fn(inputs, is_training, total_layers=total_layers, reg=reg):\n",
    "        return model_ResNetv2(inputs, is_training, total_layers=total_layers, drop_rate=drop_rate,\n",
    "                              reg=reg, num_classes=num_classes)\n",
    "\n",
    "    def optimizer_init_fn(lr):\n",
    "        return tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Stop printing from train function\n",
    "    with io.capture_output() as captured:\n",
    "        [acc_val, _] = train(model_init_fn, optimizer_init_fn, tf.nn.sparse_softmax_cross_entropy_with_logits, learning_rate,\n",
    "                             num_epochs=num_epochs, experiment_name=name)\n",
    "    # optimise one_out accuaracy\n",
    "    return -acc_val[1]\n",
    "\n",
    "\n",
    "search_result = gp_minimize(func=op_acc, dimensions=dimensions, n_calls=num_calls,\n",
    "                            verbose=True, n_restarts_optimizer=20)\n",
    "plot_convergence(search_result)\n",
    "print(search_result.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot of how the validation accuracy increased during our hyperparameter optimisation for the cross entropy loss.\n",
    "\n",
    "![Convergence plot](convergence_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the model with the validation data included in the training dataset and run the models with their optimised hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train, X_val))\n",
    "y_train = np.concatenate((y_train, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of training\n",
      "Accuracy on the test dataset\n",
      "Exact: 0.412568306010929\n",
      "1 out: 0.6065573770491803\n",
      "Top 3: 0.6584699453551912\n"
     ]
    }
   ],
   "source": [
    "# Earth Mover\n",
    "[learning_rate, reg] = [2e-5, 5e-4]  # search_result.x\n",
    "\n",
    "name = (f\"climbing_ResNet{total_layers}_lr{learning_rate}_reg{reg}\"\n",
    "        f\"_drop{drop_rate}_adam_em_test\")\n",
    "\n",
    "\n",
    "def model_init_fn(inputs, is_training, total_layers=total_layers, reg=reg):\n",
    "    return model_ResNetv2(inputs, is_training, total_layers=total_layers, drop_rate=drop_rate,\n",
    "                          reg=reg, num_classes=num_classes)\n",
    "\n",
    "\n",
    "def optimizer_init_fn(lr):\n",
    "    return tf.train.AdamOptimizer(lr)\n",
    "\n",
    "\n",
    "_, acc_test = train(model_init_fn, optimizer_init_fn, sparse_earth_mover, learning_rate, num_epochs=num_epochs,\n",
    "                    experiment_name=name, val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of training\n",
      "Accuracy on the test dataset\n",
      "Exact: 0.38524590163934425\n",
      "1 out: 0.7049180327868853\n",
      "Top 3: 0.7486338797814208\n"
     ]
    }
   ],
   "source": [
    "# CJS\n",
    "[learning_rate, reg] = [0.02, 5e-5]\n",
    "\n",
    "name = (f\"climbing_ResNet{total_layers}_lr{learning_rate}_reg{reg}\"\n",
    "        f\"_drop{drop_rate}_adam_CJS_test\")\n",
    "\n",
    "\n",
    "def model_init_fn(inputs, is_training, total_layers=total_layers, reg=reg):\n",
    "    return model_ResNetv2(inputs, is_training, total_layers=total_layers, drop_rate=drop_rate,\n",
    "                          reg=reg, num_classes=num_classes)\n",
    "\n",
    "\n",
    "_, acc_test = train(model_init_fn, optimizer_init_fn, CJS, learning_rate, num_epochs=num_epochs,\n",
    "                    experiment_name=name, val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of training\n",
      "Accuracy on the test dataset\n",
      "Exact: 0.4180327868852459\n",
      "1 out: 0.6530054644808743\n",
      "Top 3: 0.7404371584699454\n"
     ]
    }
   ],
   "source": [
    "# Cross Entropy\n",
    "[learning_rate, reg] = [5e-5, 5e-4]\n",
    "\n",
    "name = (f\"climbing_ResNet{total_layers}_lr{learning_rate}_reg{reg}\"\n",
    "        f\"_drop{drop_rate}_adam_cross_test\")\n",
    "\n",
    "\n",
    "def model_init_fn(inputs, is_training, total_layers=total_layers, reg=reg):\n",
    "    return model_ResNetv2(inputs, is_training, total_layers=total_layers, drop_rate=drop_rate,\n",
    "                          reg=reg, num_classes=num_classes)\n",
    "\n",
    "\n",
    "_, acc_test = train(model_init_fn, optimizer_init_fn, tf.nn.sparse_softmax_cross_entropy_with_logits,\n",
    "                    learning_rate, num_epochs=num_epochs, experiment_name=name, val=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "notify_time": "10"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
